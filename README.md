# -GPTQ-
现有GPTQ作为主流后训练量化方法，虽能在4bit量化下保持较高精度，但存在两大缺陷：一是固定批处理大小未考虑序列长度差异，易引发显存溢出或浪费，利用率低；二是统一量化精度未区分Transformer层功能，核心注意力层因量化误差损失性能，冗余前馈层未充分发挥压缩潜力。其他量化方法性能损失大，蒸馏法则训练成本高。 为此，本项目提出两项改进：针对批处理瓶颈，设计动态批处理策略，依据输入序列总令牌数自适应调整批次构成，最大化显存利用率；针对量化精度问题，提出层自适应量化方案，按Transformer层类型（注意力层/前馈层）分配差异化量化位数，控制核心层性能损失的同时提升压缩效率。通过联合优化，实现模型推理效率与性能的协同提升。
